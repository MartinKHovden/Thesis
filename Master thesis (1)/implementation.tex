\section{Julia Language}
Julia was the chosen language for implementing the algorithms. In addition, the code was compared to already implemented simulation tools in written in C++. The choice of programming language came down to wanting ease of use, as well as speed. Julia seemed to be the perfect choice, as the simplicity of writing code was close to Python, while the speed could be close to code written in C. 
\\
\\
One of the more popular methods for developing scientific software has been to use two languages. Either by building a prototype in a scripting language like Python, and then later implement it in a compiled language. Another method has been to use a scripting language like Python, and then calling functions from compiled languages like C to do the heavy computations. By using Julia, it should be possible to keep to only one language, and do the hole development process in only one single language. Parts of this thesis will focus on comparing the code implemented in Julia to code implemented in C++. Both speed and workflow will be compared. We will start by looking at some simple examples to see why Julia looks like a promising alternative to the more standard workflows, before we dive into the more advanced implementation of the VMC calculations. 
\\
\\
For instructions on how to download and an introduction to Julia, see \url{julialang.org}. Below, a compiled version of the introduction to Julia is presented.
\\
\\
We will see that since Julia was designed with scientific computing in mind, many operations can be done without having to use external libraries...
\subsection{Introduction to Julia}
Julia is very similar to Python. It can be written almost exactly the same way, with only small changes to the syntax. You don't have to say anything about types if you don't want to.
\\
\\
Julia can be used both in the REPL and written in a .jl-file and then ran using the julia filename.jl in the command line. 
\\
\\
In the following sections, the basics of Julia is explained. The focus is on the parts that are the most relevant to the code used for doing the simulations.  
\subsubsection{Variables}
Variables are defined as
\begin{lstlisting}
julia> x = 5.0
5.0

julia> y = 4.0
4.0

julia> x*y
20.0

julia> x+y
9.0

julia> z = x/y
1.25


julia> println(z)
1.25
\end{lstlisting}
\subsubsection{Arrays and matrices}
Arrays and matrices are important parts of scientific computing. Julia have functionality built straight into the language.
\\
\\
To construct an matrix of zeros, we can use
\begin{lstlisting}
julia> x = zeros(Float64, (2,3))
2Ã—3 Array{Float64,2}:
 0.0  0.0  0.0
 0.0  0.0  0.0
\end{lstlisting}
which creates a 2x3 matrix. Elements are stored columnwise. Indexing into the matrix can be done by using []. Sometimes, it is useful to use views instead of indexing to avoid allocating new memory every time. This will be used in the code later to improve performance. 
\subsubsection{Mathematical operations}
and we can to all mathematical standard mathematical operations like usual
\begin{lstlisting}
z = x + y
\end{lstlisting}
The results can be printed using
\begin{lstlisting}
println(z)

9.0
\end{lstlisting}
\subsubsection{Functions}
Arguements in Julia are passes by reference and not by value. 
A function is defined as
\begin{lstlisting}
function foo(x, y)
    return x + y
end
\end{lstlisting}
and they are called using
\begin{lstlisting}
z = foo(3, 4.0)
\end{lstlisting}
Note that the arguments can be of type int and float.
\subsubsection{Structs}
Structs can be used to include many object-oriented consepts into the Julia language. Julia don't have normal classes like Python or C++. A struct can be defined as
\begin{lstlisting}
struct nameOfStruct
    attribute1
    attribute2
end
\end{lstlisting}
Here a struct with 2 attributes are defined. It can be initialized by 
\begin{lstlisting}
object1 = nameOfStruct(1.0, "test")
\end{lstlisting}
Note that we can initialize both attributes with any type. We can define a new method where we let the compiler know what types each argument can take:
\begin{lstlisting}
struct foo
    att1::Int64
    att2::Float64
\end{lstlisting}
and can now be called by
\begin{lstlisting}
object2 = foo(1, 1.0)
\end{lstlisting}
Trying to initialize this struct with other types in the arguments would lead to an error. 
\subsubsection{Multiple Dispatch}
An example of multiple dispatch can be a function for calculating the area of geometric figures. We can then have a function with different methods for each geometric shape we want to include. Lets say we define two types, triangle and rectangle:
\begin{lstlisting}
struct triangle
    height
    width
end 

struct rectangle
    height
    width
end
\end{lstlisting}
Since the formula for the area of each geometric shape is different, we need to different methods for calculating it. However, we would want them both to have the same name, calculateArea. In Julia, this can be done by using multiple dispatch:
\begin{lstlisting}
function area(figure::triangle)
    return 0.5*figure.height*figure.width
end

function area(figure::rectangle)
    return figure.height*figure.width
end
\end{lstlisting}
This will be very useful when implementing the full code for the VMC simulations, as we will see in a later chapter. 

\subsection{Speed of Julia}
The speed of a programming can be looked at from 2 main sides. First, it is the speed of the computations, secondly, it is the speed of the development.
\\
\\
In this section, we will look at some simple examples on how Julia's computational speed compares to Python and C++. One of the main problems with Python is the slow speed of loops. We will see that a speed-up of 100x over Python is possible by using Julia. C and C++ are known for being very fast. It has been reported that it is possible to get speed close to both of them by using Julia. We will see that for some simple examples, we get performance very close to C and C++. Later, we will also test for the full simulations. \\
\\
Below, the results for running a Monte Carlo cycle for N particles are shown. The system is a system 2 bosons in 2 dimensions. The code is based on the lecture notes by Morten Hjorth Jensen, and rewritten to Julia to compare the two languages. 

\subsection{Flux - Machine Learning in Julia}
One of the most popular machine learning libraries in Julia is Flux. Flux is a library written completely in Julia, which makes it easy to take derivatives, and work with, any expression written in Julia. One of the main features is that it can be used to take gradients of any function written in pure Julia. 
\\
\\
We will also use the library Zygote for doing automatic differentiation. 
\\
\\
I refer to the Flux guide for more details on Flux, but some of the main parts will be presented here: 
\\
\\
A simple neural network in Flux can be implemented as
\begin{lstlisting}
using Flux
model = Chain(Dense(10, 100, sigmoid), Dense(100, 1, tanh))
\end{lstlisting}
\subsection{Automatic differentiation}
Automatic differentiation will be used to calculate the gradients and laplacian of the neural network. Automatic differentiation can easily be done in Julia by using the Flux machine learning package. Flux has functions for both the gradient and the hessian of the network, and they can be applied to any function in Julia.
\\
\\
Flux uses reverse automatic differentiation.
\\
\\
One of the main drawbacks of using automatic differentiation is that it is not as fast as using analytical expressions. However, using autograd will provide us with much more flexibility when it comes to using more advanced network structures. Since we for now will work with quite small systems, autograd will work fine to illustrate the effectiveness of the neural network wave function. However, for now, autograd scales quite badly for larger input, so for larger systems, it will be advised to use analytical expression. However, I hope that new and more effective methods will be found to solve the speed problem.

\subsection{Random Numbers in Julia}
Julia has advanced options for random numbers generation. We can use the Random package, and use the standard methods for drawing normally distributed numbers
\begin{lstlisting}
using Random

samples = randn(100)
\end{lstlisting}
which returns an array of 100 normally distributed numbers. We can also choose more state-of-the-art numbers generators. Julia uses the MersenneTwister generator as standard, but we can also choose the generator actively. By first initializing a MersenneTwister object, we can send it in as an argument 
\begin{lstlisting}
using Random

rng = MersenneTwister(1234)
samples = randn(100, rng)
\end{lstlisting}

In general, when choosing a random number generator, we want the generator to produce uncorrelated samples with a long period. 

\newpage

\section{Implementation of framework}
In this section some practical tips and tricks used when implementing the framework in Julia is presented. One of the main problems with VMC calculations is the computational cost. Often, we need to collect millions of samples, and for each sample we have to have to do calculations. In total, this process can be very tedious. Therefore, it is important to make each step run as efficient as possible. In this part, we will present techniques that we will use to do this, in addition to making the code more flexible and easier to extend. 
\subsection{Wavefunction}
In our case, the wave function can be seen as a product of wave function elements. Each element can be used separately, but combined we hope to get better approximations of the actual system. In later sections, we will present how we can work with each wave function element. In this section we will present a few different elements that we will test: The slater determinant, the Jastrow factor, a nerual network, and a resticted Boltzmann machine. 
\\
\\
Before we look at how to handle the different wave function elements, we will note that 
\begin{equation}
    
\end{equation}
\\
\\
To make it easier to combine the different wave function elements we will work with the logarithm of the wave function. We note that the term in the hamiltonian, 
\begin{equation}
    \frac{1}{\psi}\frac{\partial \psi}{\partial x} = \frac{\partial}{\partial x}\log \psi
\end{equation}

This way, we can easily split the wavefunction into parts, where each part can be handled on its own. This will also make it much easier to extend the code with new wavefunction elements, as well as calculating all the quantities needed in the simulation. By using the identity of logarithms
\begin{equation}
    \log(\prod_i x_i) = \sum_i \log x_i
\end{equation}
Since the wavefunction can be written as a product of wave function elements $\psi_i$, 
\begin{equation}
    \psi_T = \prod_i \psi_i 
\end{equation}
we see that
\begin{equation}
    \log \psi_T = \sum_i \log \psi_i
\end{equation}
This is beneficial when taking the derivatives and second derivatives. We can then just add the contributions to the derivatives from each wave function element separately. 
\\
\\
Now, we note that 
\begin{equation}
    \frac{1}{\psi_i}\frac{\partial \psi_i}{\partial x} = \frac{\partial }{\partial x}\log\psi_i.
\end{equation}
We can then replace the terms in the hamiltonian with the log terms, and write
\begin{equation}
    \frac{1}{\psi_T}\frac{\partial \psi_T}{\partial x} = \frac{\partial}{\partial x}\log \psi_T = \frac{\partial}{\partial x}\sum_i \log \psi_i
\end{equation}
where $x$ is some parameter in the wave function. We can do similarly for the second derivative, and see that
\begin{equation}
    \frac{1}{\psi_T}\frac{\partial^2 \psi_T}{\partial x^2} = 
\end{equation}
\\
\\
The problem of calculating the full gradient and laplacian of the trial wave function is now reduced to calculating the gradients and the laplacians of each wave function element.
\\
\\
In addition to calculating the gradient and laplacian of the wave function elements, we need to be able to calculate the ratio between the new and the previous wave function value. We will see that we can find efficient methods for calculating the ratios, without actually having to calculate the value of the wave function. This is beneficial since some of the wave function elements can be quite heavy to compute. 
\subsection{Slater determinant}
A description of the Slater determinant can be found in the theory chapter. Here we will focus on how to efficiently implement the operations on the Slater determinant in Julia. 
\\
\\
The two main bottlenecks with the Slater determinant is the computation of the inverse and the determinants. This is expensive operations using classical numerical procedures, so we want to find more efficient ways of computing them. One way to do this is to utilize the fact that we are only moving one particle at a time. 
\\
\\
As before, we let the matrix S be the slater matrix, given by
\begin{equation}
    S = 
    \begin{bmatrix}
    \phi_1(r_1) & \phi_2(r_1) & \phi_3(r_1) & ... & \phi_N(r_1) \\
    \phi_1(r_2) & \phi_2(r_2) & \phi_3(r_2) & ... & \phi_N(r_2) \\
    ... & ... & ... & ... & ... \\
    \phi_1(r_N) & \phi_2(r_N) & \phi_3(r_N) & ... & \phi_N(r_N) \\
    \end{bmatrix}
\end{equation}
for a N particle system. The matrix consist of single particle wave functions, given by the Hermitian polynomials. We note that we can split the matrix into a spin up and a spin down part, and each part can then be handled separately. In the slater matrix, the $\phi$ is the single particle wave function, given by a combination of the Hermitian basis functions:
\begin{equation}
    H_{nx} = ...
\end{equation}
To make the derivations easier, we can split the functions into the exponential part and the Hermitian part. This way we can handle each part on its own, and we do not really need... 
\\
\\
One important feature of the slater determinant, is that when moving only one particle, only one row of the matrix is changed. This will be used to find efficient methods for finding the various quantities needed in the simulations. 
\subsubsection{Calculating the ratio}
One of the steps we will use to increase the speed of the calculations is find a effective way of calculating the ratio when only moving one particle. the reason why this ois such an expensive operation is that to calculate the determinant of a N x N matrix, we need $O(N^3)$ operations. This can easily be done by taking a dot-product of the i'th row of the slater matrix with the new coordinates and the i'th column of the old inverse matrix, which will decrease the time-complexity to $O(N^2)$.  
\\
\\
to calculate the ratio when moving particle i, we get the formula

\begin{equation}
    R = \sum_j \phi_j(r_i^\text{new})d_{ij}^{-1}(r^\text{old})
\end{equation}




\subsubsection{Calculating the derivatives}
\begin{equation}
    \frac{\partial S}{\partial x_i}
\end{equation}
We see that to calculate the gradient of the slater determinant with respect to the i'th particle, we need the gradient of the i'th single particle wave function as well as the inverse of the slater matrix. 
\subsubsection{Calculating the double derivatives}
For the double derivatives, we get similar expression as for the single derivatives. However, we now need the double derivatives of the single particle wave function instead. 
\subsubsection{Updating the inverse matrix}
Another expensive operation is the calculations of the inverse of the slater matrix. Again, we will use the fact that we only move one particle at a time, and find an effective method to updated only what is needed in this case. 



\subsection{Jastrow Factor}
The Jastrow factor is the part of the wavefunction that we use to model the electron-electron correlation. We will use a simple Jastrow factor. There exists many different proposed such correlation factors. Common for many of them is the goal to make it fast to compute the different ratios and derivatives that are used in VMC calculations \cite{}. 
The first Jastrow factor we will use is the simple one given by
\begin{equation}
    \psi(\boldsymbol{r}, \boldsymbol{\beta}) = \exp\left(\sum_{i<j}f_{ij}\right)
\end{equation}
where we will use
\begin{equation}
    f_{ij} = \beta_{ij}r_{ij}.
\end{equation}
Other alternatives exists as well. 
Here we have that $r_{ij}$ is the distance between particle i and j, and $\beta_{ij}$ is a matrix of variational parameters. We then want to find the optimal $\beta_{ij}$ values, i.e the values that minimize the energy of the system. 
\subsubsection{Calculating the ratio}
First, we note that we can easily calculate the ratio when we assume that only one particle, particle i, is moved at each monte carlo step. This way we get that
\begin{equation}
    R = \frac{|\psi^{\text{new}}|^2}{|\psi^{\text{old}}|^2} = \exp\left(2\sum_{i<j} f_{ij}^\text{new} - 2\sum_{i<j} f_{ij}^\text{old}\right) 
\end{equation}
Now, we can note that since we only move particle i, all terms except for the ones including particle i disappears, and we are left with the simple expression
\begin{equation}
    R = \exp\left(2\sum_{j\neq i}(f_{ij}^\text{new} - f_{ij}^\text{old})\right) 
\end{equation}
which can be calculated quite fast. Again, we assumed that only the i'th particle was moved. 

\subsubsection{Calculating the derivatives}
As for the Slater determinant, we need to find the derivative and double derivative with respect to each particles dimension, as well as the gradient with respect to the variational parameters. 
\\
\\
We will start by finding the gradient with respect to the particles positions. Taking the derivatives, we note that we have N-1 terms that are affected by the derviatives. We now let $(\boldsymbol{r}_k)_l$ be the l'th component of particle k. This way we can write the derivative with respect to the component as
\begin{equation}
    \frac{\partial \psi}{\partial (\boldsymbol{r}_k)_l} = \frac{\partial}{\partial (\boldsymbol{r}_k)_l}}\left(\sum \sum \beta_{ij}r_{ij}\right)\exp\left(\sum \sum \beta_{ij} r_{ij}\right)
\end{equation}
We now need to find the derivatives of the first part of the previous equation
\begin{equation}
    \frac{\partial}{\partial (\boldsymbol{r}_k)_l}}\left(\sum \sum \beta_{ij}r_{ij}\right) = \sum_j\frac{\beta_{ij}}{r_{ij}}
\end{equation}
\subsubsection{Calculating the double derivatives}
Next, we find the double derivatives by taking the derivatives one more time
\begin{equation}
    
\end{equation}

\subsubsection{Calculating the derivatives with respect to variational parameters}
Lastly, we take the derivative with respect to the variational parameters. Since each variational parameter only are connected to the corresponding distance $r_{ij}$, we get that the parameter derivatives are given by.
\begin{equation}
    \frac{\partial}{\partial \beta_{ij}}\ln(\psi) = \frac{\partial}{\partial \beta_{ij}}\sum_{i<j}f_{ij} = r_{ij} 
\end{equation}
since every other term in the sum is independent of $\beta_{ij}$

\subsection{Restricted Boltzmann machine}
The next wavefunction we will study is the restricted boltzmann machine. The wave function value is given by
\begin{equation}
    \psi(\boldsymbol{x}) = 
\end{equation}
The restricted Boltzmann machine have the gaussian part baked into it, so it is no need to keep the gaussian part in the slater detemrinant when working with the RBM. We will see that this is not the case for neural networks. 
\subsubsection{Ratio}

\subsubsection{Gradient}
The derivatives with respect to the visible nodes are given by
\begin{equation}
    \frac{\partial}{\partial x_m}\ln \uppsi = -\frac{1}{\sigma^2}(x_m - a_m) + \frac{1}{\sigma^2} \sum_n^N \frac{w_{mn}}{\exp\left[-b_n - \frac{1}{\sigma^2}\sum_i^Mx_iw_{in}\right] + 1}
\end{equation}
 
\subsubsection{Laplacian}
and 
\begin{equation}
    \frac{\partial^2}{\partial x_m^2}\ln \uppsi = -\frac{1}{\sigma^2} + \frac{1}{\sigma^4}\sum_n^N w^2_{mn}\frac{\exp\left[b_n + \frac{1}{\sigma^2}\sum_i^Mx_iw_{in}\right]}{\left(\exp\left[b_n + \frac{1}{\sigma^2}\sum_i^Mx_iw_{in}\right]+1\right)^2}
\end{equation}
\cite{mhj_ml} and can be used for calculating the local energy.

\subsubsection{Gradient with respect to parameters}
The derivatives of the wave function with respect to the biases and weights are needed in the gradient descent updates. For a full derivation, see the appendix. Those are given by 
\begin{equation}
    \frac{\partial}{\partial a_m}\ln \uppsi = \frac{1}{\sigma^2}(X_m - a_m)
\end{equation}
\begin{equation}
    \frac{\partial}{\partial b_n}\ln \uppsi = \frac{1}{\left(\exp \left[-b_n - \frac{1}{\sigma^2}\sum_i^MX_iw_{in}\right]+1\right)}
\end{equation}
and 
\begin{equation}
    \frac{\partial }{\partial w_{mn}}\ln \uppsi = \frac{X_m}{\sigma^2\left(  (\exp \left[-b_n - \frac{1}{\sigma^2}\sum_i^MX_iw_{in}\right]+1 \right)}
\end{equation}
\cite{mhj_ml}.
\subsection{Neural network}
The last part will consentrate on representing the wavefunction using a neural network in addition to the slater matrix. We choose to use the slater determinant to handle the antisymmetric restriction for fermions and the Pauli principle. this could also have been implemented in the neural network, but this does not necessarily lead to the best results SOURCE!!!!!. 
We will choose a simple feed forward network, where we want minimize the local energy with respect to the weights of the neural network.
\\
\\
The wavefunction given by the neural network will be given by
\begin{equation}
    \psi(x) = \exp(f_{NN}) 
\end{equation}
where $f_{NN}$ is the output of the neural network. We use the exponential function to simplify the expression used in the calculation of the ratio and the various gradients. 
\\
\\
We can now go into two different directions when calculating the gradients. We can either use Automatic differentiation, or we can find analytical expression. Both will be tested, and we will note that the analytical expression provide much faster calculations. 
\\
\\
Automatic differentiation can easily be done in Julia using the Flux package. For more details on automatic differentiation, see the Julia chapter. Here, we will present the analytical expressions. 
\subsubsection{Ratio}
The ratio can be calculated as 
\begin{equation}
    R^2 = \frac{\exp(f_{NN}^\text{new})^2}{\exp(f_{NN}^\text{old})^2} = \exp(2(f_{NN}^\text{new} - f_{NN}^\text{old}))
\end{equation}
\subsubsection{Gradient}
Now, we are interested in finding the analytical expression for the gradient of the output with respect to the input. We will see that the expression is quite similar to the expression used in backpropagation expression found in chapter ....  
\\
\\
\begin{equation}
    \frac{\partial a_i^l}{\partial x_j} = \frac{\partial \sigma(z_i^{l-1})}{\partial x_j} =  \frac{\partial \sigma\left(\sum_k w_{ik} a_k^{l-1} + b_i\right)}{\partial x_j} = \sum_k w_{ik}\frac{\partial a_k^{l-1} }{\partial x_j} 
\end{equation}
The recursive relationship can be calculated by noting that
\begin{equation}
    \frac{\partial a_i^0}{\partial x_j} = \frac{\partial x_i}{\partial x_j} = 
    \begin{cases}
    1, & i=j \\
    0, & i \neq j
    \end{cases}
\end{equation}
First, we note that $ln(\psi(x)) = f_{NN}$. We can then let the output
\subsubsection{Laplacian}
The next step is to find the laplacian with respect to the inputs. We can do this by using the chain rule on the previous expression for the gradient. First, we note that for the input layer the double derivatives are equal to zero,
\begin{equation}
    \frac{\partial^2 a_i^0}{\partial x_j^2} = 0
\end{equation}
Then, we can find the more general expression for the other layers, that can calculated by using the iterative expression.

\subsubsection{Parameter derivatives}
The last step is to find the gradient of the output with respect to weights and biases of the network. Those are presented in the chapter on the backpropagation algorithm, but the final expressions are shown here:
\begin{equation}
    test
\end{equation}
For a full derivation, see chapter ... .
\subsubsection{Derivatives using automatic differentiation}
For the results we will try to use automatic differentiation as much as possible. This is because it will make the methods much more flexible compared to having to derive analytical expressions. It might be that other more complicated networks leads to better results, for example using recurrent neural networks, or more advanced layer structures. Those can easily be implemented in existing libraries.
\newpage