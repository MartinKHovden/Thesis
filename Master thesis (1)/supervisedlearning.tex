Machine learning is a general name of a broad range of algorithms used for making a computer learn to perform different tasks. Machine learning has gained a lot of popularity over the last 10 years due to an increase in computing power. Machine learning is now used in almost all parts of industry and academia, with possible applications ranging from self driving cars to simulating physical systems. Machine learning is not something new. The theory has existed for many years. However, the lack of enough computing power has restricted the actual practical usecases.  
\\
\\
The algorithms are often separated into classes, classified by how they learn. The three main classes are supervised learning, unsupervised learning, and reinforcement learning. In this text we will mainly focus on the first two, but interesting application to physics can also be found for reinforcement learning\cite{}. The main difference between the two is how they learn. Supervised learning, as the name suggests, needs to be supervised during training. This is usually done by letting the model train on a data-set where the target values are known. Unsupervised, on the other hand, are able to learn various properties about some input. We will come back to this in a later chapter. 
\\
\\
This chapter will give a gentle introduction to machine learning and how the algorithms works under the hood. We will start with a description of linear regression, before moving on to logistic regression. Both methods introduce important concepts that most machine learning algorithms rely on. After that we will introduce restricted Boltzmann machines and neural networks. 
\\
\\
In the simplest form, machine learning can basically be reduced to an optimization problem where you tweak the parameters of the model to give the desired results. 
\\
\\
Exploration vs exploitation. 
\section{Supervised learning}
Supervised learning is a type of machine learning where we make the computer learn from data where the data contains both the predictors and the targets. The goal is then to minimize the difference between the output of the machine learning model and the target, when the predictors are fed into the model. The goal is to train the model on a set of training data, so that the model is able to perform well on new data that the model has never seen before. This is often called to make the model ... To be able to quantify the difference between the the output and the targets, we need some kind of distance measure.  
\\
\\
The main focus in this section will be on neural networks and restricted Boltzmann machines, since they will play an important part in the later work. However, regression methods is presented first because they give a good introduction to how machine learning actually works under the hood. It will also give us some intuition into how we can teach the computer to solve various tasks, sometimes even better than humans. Neural network are often called a black box where you give it some input and it produces some output. However, when we understands how it works, we note that it is just an optimization problem. 
\\
\\
 
\subsection{Ordinary Least Square Regression}
The ordinary least square method is a popular method for solving machine learning problems. Even though the method is simple and easy to implement, it often gives very good results in real world applications. The method can be used to model both linear and polynomial relationships in the data. For regression it is possible to find a closed form analytical expression for the best parameters of the model. This will make it a good starting point for developing an intuition for how machine learning algorithms actually learn.
\\
\\
With ordinary least square regression the aim is to find a relationship between a response variable and a set of corresponding covariates. The response variable can be thought of as input to the model. The predictor variable is what we want the model to spit out, given some input. The goal is to train the model on a labeled data set, so that the model is able to accurately predict the outcome, given some some new, unseen, input. 
\\
\\
More formally, we want to find a model for the expected value of the response variable, given some inputs.
\begin{equation}
\text{E}[y|x_1, x_2, ... , x_n]
\end{equation} 
For ordinary least squares, we assume that our data follows a function 
\begin{equation}
    \boldsymbol{y} = \boldsymbol{f}(\boldsymbol{X}) + \boldsymbol{\epsilon},
\end{equation}
where $\boldsymbol{f}$ is a non-random function of the predictors and $\boldsymbol{X}$ is a matrix with data points. We call $\boldsymbol{X}$ the design matrix, and $\boldsymbol{y}$ is a vector of response variables. Our goal is to find an approximation of $\boldsymbol{f}$. 
We will start by looking at the linear regression case. In linear regression, we assume a linear relationship between the response variable and the covariates, so a natural choice is to try to find a function $\boldsymbol{f}$ such that 
\begin{equation}
    \boldsymbol{y} = \beta_0 + \boldsymbol{x_1}\beta_1 + ... + \boldsymbol{x_p}\beta_p + \epsilon
\end{equation}
where $\beta_i$ is called the regression coefficients and $\boldsymbol{x_i}$ is a vector containing samples of feature i. $\beta_0$ is called the bias term. This means that by increasing $x_i$ by one unit and let all other covariates stay fixed, \boldsymbol{y} increase by $\beta_i$. 
\\
\\
We now let $\epsilon$ be normally distributed with variance $\sigma^2$, 
\begin{equation}
    \boldsymbol{\epsilon} \sim N(0, \sigma^2).
\end{equation}
Since $\boldsymbol{f}$ is a deterministic variable, the distribution of \boldsymbol{y} is also normally distributed.
\begin{equation}
    \boldsymbol{y} = \boldsymbol{f} + \boldsymbol{\epsilon} \sim N(0, \sigma^2).
    \label{eqn:yfe}
\end{equation}
A simple way to store our data is by using an n x (p+1) matrix. We call it X:
\begin{equation}
    \boldsymbol{X} = 
    \begin{bmatrix}
    1 &x_{11} & x_{12} & ... & ... & x_{1p} \\
    1 &x_{21} & x_{22} & ... & ... & x_{2p} \\
    1&... & ... & ... & ... & ... \\
    1 & x_{n1} & x_{n2} & ... & ... & x_{np}\\
    \end{bmatrix}
    = 
    \begin{bmatrix}
    \boldsymbol{x_1} & \boldsymbol{x_2} & ... & ... & \boldsymbol{x_p}
    \end{bmatrix}
\end{equation}
By adding the column of one we are able to account for the bias term in equation \ref{}.
We will define $x_{ij}$ as the i'th sample of the j'th predictor.
Since OLS is a supervised method, we also have to have a set of target values for each sample. We define the vector $\boldsymbol{y}$ as the vector of target values. 
\begin{equation}
    \boldsymbol{y} = 
    \begin{bmatrix}
    y_1 \\
    y_2 \\
    ... \\
    ... \\
    y_n
    \end{bmatrix}
\end{equation}
where $y_i$ is the target-value for the i'th sample. Equation \ref{} can then be rewritten as
\begin{equation}
 \boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}
\end{equation}
As mentioned, a simple trick lets us model non-linear relations in the data. This is done by expanding the basis. By adding a component wise power of a feature vector, $\boldsymbol{x_i^j}$ for some $i,j\in\mathcal{R}$, to $\boldsymbol{X}$, we can also get a polynomial relationship between the response and of variable i. By adding for example $\boldsymbol{x_jx_i}$ we can add interactions between variable j and i. We can then use the same method for training polynomial regression as for linear regression, only with an extended design matrix $\boldsymbol{X}$. This makes the ordinary least squares regression able to model more complex data.
From now on we will write the estimate of $\boldsymbol{\beta}$ as $\boldsymbol{\beta}.$ Eq. \ref{eqn:yfe} can then be approximated by 
\begin{equation}
    \boldsymbol{\hat{y}} = \boldsymbol{X\beta} 
\end{equation}
where $\boldsymbol{\beta}$ is a vector that contains the regression coefficients,
\begin{equation}
    \boldsymbol{\beta} = (\beta_0, \beta_1, ..., \beta_n)^T
\end{equation}
The goal is then to find the unknown $\boldsymbol{\beta}$. This is an optimization problem, which is solved by minimizing the residual sum of squares, RSS, as a function of $\boldsymbol{\beta}$. The easiest way to do this is by rewriting the RSS on matrix form.  minimization problem as an minimization of an matrix equation.  
\begin{equation}
    \text{RSS}(\beta) = \sum_i(y_i - \hat{y}_i)^2 = (\boldsymbol{y - X\beta})^T(\boldsymbol{y - X\beta}).
\end{equation}
Taking the derivative with respect to $\beta$, and setting equal to zero
\begin{equation}
    \frac{\partial\text{RSS}(\beta)}{\partial \beta} = -2\boldsymbol{X}^T(\boldsymbol{y - X\beta}) = 0.
\end{equation}
By rearranging the equation, the expression for the coefficients are $\boldsymbol{\beta}$:
\begin{equation}
    \boldsymbol{\beta}_{\text{OLS}} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation}
In the code, we have accounted for one of the main problems with fitting ordinary least squares. This occurs when we encounter singular or nearly singular matrices. We then get problems with the inversion. One way to fix this problem is by using regularization techniques that will be discussed later.  However, in the code we have fixed the problem by using numpy.pinv instead of numpy.inv. numpy.pinv uses a singualar value decomposition and calculates the so-called pseudo-inverse. For more information about the techniques, see \cite{hastie}.
\\
\\
Since the betas have some uncertainty, we are unable to know the exact value of each coefficient. Therefore it is interesting to look at the confidence intervals for the coefficients.  We start by finding the variance for each coefficient with the formula
\begin{equation}
    \text{Var}(\boldsymbol{\beta}_{\text{OLS}}) = \text{Var}((\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}) = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\text{Var}(\boldsymbol{y})((\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T)^T = \sigma^2 (\boldsymbol{X^TX})^{-1}.
\end{equation}
The last equality comes from the fact that $\text{Var}(y) = \text{Var}(\boldsymbol{f}) + \text{Var}(\boldsymbol{\epsilon}) = \sigma^2$, since $\boldsymbol{f}$ is non-stochastic and the variance of $\boldsymbol{\epsilon}$ is $\sigma^2$.
\\
\\
We can see that the distribution of the betas are given by 
\begin{equation}
    \boldsymbol{\beta} \sim \mathcal{N}(\boldsymbol{\beta}^{\text{exact}}, \sigma^2(\boldsymbol{X^TX})^{-1}). 
    \label{eq:beta_dist}
\end{equation}
\\
Now that we know the variance of each coefficient, we can create confidence intervals for each one of them. The variance of the coefficients are found on the diagonal of the covariance matrix in equation \ref{eq:beta_dist}. The 95\% confidence interval for coefficient i is given by 
\begin{equation}
    \left[\beta_i - 1.96\sqrt{\text{Var}(\beta_i)}, \beta_i + 1.96\sqrt{\text{Var}(\beta_i)}\right]
\end{equation}
\\

\subsection{Numerical optimization}
As we saw in the previous chapter, it is possible to train some machine learning models by finding closed form solutions for the optimal parameters. However, this is not the norm. Most algorithms requires us to find the optimal parameters in the model numerically. Numerical optimization is a broad field with numerous different algorithms and applications. Numerical optimization is an iterative process, where the gradient of the function is used to progressively get better and better estimates of the model parameters that minimize the function value. 
\\
\\
A minimization problem can be stated as: Given a function $f(\boldsymbol{x})$, we want to find the $\boldsymbol{x}_0$ s.t $f(\boldsymbol{x}_0) \leq f(\boldsymbol{x})$ for all $\boldsymbol{x}$. The goal of numerical optimization is to find an approximation to $\boldsymbol{x}_0$. Mathematically, it is often written as:
\begin{equation}
    \text{min} f(\boldsymbol{x})
\end{equation}
s.t some restrictions are fulfilled.
\\
\\
When fitting machine learning models the goal is to minimize a cost function with respect to the models parameters. The cost function says something about how wrong our models output is compared to the desired outputs. Cost functions will be discussed in more detail later. Some machine learning models have an analytical expression for finding the optimal values. In general, this is not the case.
Finding the optimal parameters then requires numerical optimization. The search for the optimal parameters are typically the bottleneck of machine learning algorithms \cite{Hands-On}. Finding effective methods for obtaining minimas is therefore of great interest. With the increase in computational power over the last years, training larger and larger models have become possible. In this part, numerical optimization is discussed in general, before being linked to machine learning later. 
\subsubsection{Gradient descent}
Gradient descent is one of the simplest numerical methods for finding the optimal values in a minimization problem. It is an iterative method, and it is often used in machine learning to find the optimal weights and biases of the model. Many other algorithms can be seen as extensions of gradient descent, and it gives a good intuition on how a large class of algorithms actually works. This makes it a good starting point in the study of numerical optimization. In addition, it will be used to train our model later.
\\
\\
The idea behind the gradient descent method is to, for each iterative step, calculate the gradient of a function with respect to the parameters, and then move the parameters a given distance in the opposite direction \cite{NumOpt} of the gradient. The intuition behind this is that by moving in the opposite direction of the gradient we will move in the direction where the function value decreases fastest. By continuously moving in the opposite direction of the gradient for given length, the idea is to end up in the minimum of the function. 
\\
\\
Mathematically, the gradient descent method can be summarized in the formula
\begin{equation}
    \boldsymbol{x}^{k+1} = \boldsymbol{x}^k - \alpha_k \nabla \boldsymbol{f}(\boldsymbol{x}^k)
\end{equation}
where $\boldsymbol{x^k}$ is the parameters we want to change, $\alpha_k > 0$ is the step-length, $\boldsymbol{x} = (x_1, x_2,...,x_n)$ and $\nabla \boldsymbol{f}(\boldsymbol{x^k})$ is the gradient of the function. 
\\
\\
The approximation of the minimum is in general sensitive to the choice of initial condition $\boldsymbol{x}^0$. If the initial choice is bad, the algorithm can converge to local minimums and get stuck there \cite{NumOpt}. 
\\
\\
The next question is how to we know when to stop the method. There are two main ways of stopping it. The first one is to set a predefined number of iterations and just let the algorithm run through. The other way is to check is the norm of the gradient is smaller than some tolerance $\epsilon$. We then want to check if
\begin{equation}
    ||\nabla \boldsymbol{f}(\boldsymbol{x}^k)|| \leq \epsilon
\end{equation}
A combination of both is also an alternative. We then stop the algorithm either when the norm is smaller than the tolerance or when the predefined number of iterations has been reached.
\\
\\
For the simulations done in this thesis we will stick to a predetermined number of iterations. This is to better be able to analyze the convergence properties of the chosen optimization algorithm. 
\\
\\
In machine learning, the step length is often called the learning rate. It determines how fast the model learn. However, the step-length have to be chosen carefully, and there are different ways of doing this. One method is to use a predefined value. Choosing it to low will lead to very slow convergence. Choosing it to large can lead to the approximation bouncing around the actual solution or diverging away from the solution. By using a to large value, we are no longer sure that the next parameters will lead to a smaller function value.  By using a pre-specified value, we therefore have to test for different values and see which one works best for our problem. Alternatively, the step length can be chosen by minimizing the function in the direction of the gradient. The last method is called a line search. In this project we will use a predetermined learning rate and test for different values to see how it effects the performance of the model. 
\\
\\
Another important aspect of numerical optimization is how they handle multi modal functions. The general gradient descent method can get troubles for ill-behaved functions with many local minimums and "rough terrain". Since the method is deterministic, the method can get stuck in local minimums and in general we will have no idea whether the point is a global or a local minimum. The initial choice of parameters is therefore important. However, if the function is convex and differentiable, every minimum is a global minimum and $\nabla \boldsymbol{f}(\boldsymbol{x}) = 0$.\cite{NumOpt} 
\\
\\
In machine learning, the function to minimize is the cost function. If the parameters is denoted by $\boldsymbol{\theta}$ and the cost function by $\boldsymbol{C}$, we get that 
\begin{equation}
    \boldsymbol{\theta}^{k+1} = \boldsymbol{\theta}^k - \alpha \nabla_\theta \boldsymbol{C}(\boldsymbol{\theta}^k).
\end{equation}
where $\nabla_\theta \boldsymbol{C}(\boldsymbol{\theta}^k)$ is the gradient with respect to the parameters.
The cost function is the sum of the loss function for each sample
\begin{equation}
    C(\boldsymbol{\theta}) = \sum_i^n \boldsymbol{c}_i(\boldsymbol{x}_i, \theta)
\end{equation}
This can make the gradient descent step very expensive to compute for large data sets with many iterations. One way to fix this is by only using a small batch of the samples for each calculation of the gradient. These batches are called "mini-batches". 

\subsubsection{Momentum gradient descent}
Momentum gradient descent is another popular optimization algorithm that are widely used in machine learning. It is an extension to the gradient descent method, in that it tries to take into account the direction of the last step. This is where the momentum name comes from. 

\subsubsection{Stochastic gradient descent (SGD)}
Stochastic gradient descent is another extension of the gradient descent method. The idea behind stochastic gradient descent is to only use a subset of the data for calculating the gradient at each iterative step. By introducing stochastisity we can try to improve on some of the shortcomings of the general gradient descent method. By using only a fraction of the data points, we will hopefully be able to escape local minimums and be able to explore other areas of the parameter space. Since the total number of gradients to calculate is smaller, the computational cost will be smaller compared to a full gradient descent. The speed of the optimization problem is not a big issue in VMC, since most of the time is used in the MCMC-steps. However, for machine learning algorithms where you want to train the model for millions of steps, this can be a great benefit. 
\\
\\
If we have n data samples, we can split the data into K batches. The i'th batch is denoted by $B_i$ and consist of n/K data points. We can then find the gradient for this batch by
\begin{equation}
    \nabla_{B_k}\boldsymbol{C}(\boldsymbol{\theta}) = \sum_{i \in B_k}\nabla c_i(x_i, \boldsymbol{\theta})
\end{equation}
where $c_i$ is the loss function for the i'th sample. 
The next step can then be calculated as 
\begin{equation}
    \boldsymbol{\theta}^{k+1} = \boldsymbol{\theta}^k - \alpha \nabla_{B_k}\boldsymbol{ C}(\theta^k).
\end{equation}
For each step we choose the k randomly. We have two extra parameters that have to be chosen when moving from gradient descent to stochastic gradient descent. The batch size and the number of epochs. One epoch is finished when we have iterated K steps, so that all the data have had the opportunity to help with upgrading the parameters. We typically run many epochs and the number of epochs needed varies from problem to problem. 
\\
\subsubsection{Adam}
The Adam algorithm is another optimzation method. It was presented by D. P. Kingma and J. L. Ba in a paper in 2014, \url{https://arxiv.org/pdf/1412.6980.pdf}. It is a first-order stochastic optimization method. As for the gradient descent method, it only requires first-order gradients. The idea is to use different learning rates for each of the parameters that should be optimized. The learning rates are adaptive as well. The idea comes from combining the best functionality of AdaGrad method and the RMSProp algorihtm.
\subsection{Logistic regression}
Logistic regression is a method for finding the probability of a data point belonging to a certain class.  Given a set of data points $(y_i, x_i)$, we want to classify the input, $x_i$, into at set of K classes, $y_i \in (1,2,...,K-1)$. In this report we will look at binary classification problems. This is problems with only two classes, so $y_i \in (0,1)$. The probabilities of each class is given by
\begin{equation}
    p(y = 1 | \boldsymbol{x}, \boldsymbol{\beta}) = \frac{1}{1 + \exp(-\boldsymbol{\beta}^T\boldsymbol{x})},
\end{equation}
\begin{equation}
    p(y = 0|\boldsymbol{x}, \boldsymbol{\beta}) = 1 -   p(y = 1 | \boldsymbol{x}, \boldsymbol{\beta})
\end{equation}
where $\boldsymbol{\beta} = [\beta_0, \beta_1, ..., \beta_p]$ is the parameter vector, and $\boldsymbol{x} = [1, x_1, x_2, ..., x_p]$.\cite{Hands-On} The 1 is added as an intercept. Logistic regression is called a soft-classifier. In contrast to a hard classifier that returns the predicted class, a soft classifier returns the probability of belonging to each class. When the probability of the class is calculated, we can classify the points given a threshold. Samples with probability higher than the threshold is classified as 1, and  samples with probability lower than the threshold is classified as 0. A typical threshold value is 0.5. The class is then given by 
\begin{equation}
   y =  \begin{cases}
    1 & \text{if } p(y = 1 | \boldsymbol{x}, \boldsymbol{\beta}) \geq 0.5 \\
    0 & \text{if } p(y = 1 | \boldsymbol{x}, \boldsymbol{\beta}) < 0.5
    \end{cases}
\end{equation}
\\
\\
Threshold values can take on values in the interval [0,1]. Different problems require different threshold values. For example, for some problems there is important to not have false negatives.  This can be for medical purposes, where we don't want to classify a sick person as a healthy person. Lowering the threshold can then lower the chance of this. However, choosing it to low might lead to classifying a lot of healthy people as sick. There have to be a balance between the two.  
\\
\\
The problem is finding the optimal parameter vector $\boldsymbol{\beta}$. A often used method  maximum likelihood estimation. 
The maximum likelihood is given by 
\begin{equation}
   P(\mathcal{D|\beta}) = \prod_{i=1}^n[p(y_i = 1|x_i, \beta)]^{y_i}[1 - p(y_i = 1|x_i, \beta)]^{1 - y_i}.
\end{equation} \cite{hastie}
We want to maximize this function. By taking the log, we get 
\begin{equation}
    \sum_i^n y_i \left(\log[p(y_i=1|x_i, \beta) + (1-y_i)\log [1 - p(y_i = 1|x_i, \beta)]\right)
\end{equation}
Taking the log does not change the maximum of the function. Finding the parameters that maximizes this is the same as finding the parameters that minimize the negative. The methods from the optimization chapter can be used. The cost function can then be expressed as
\begin{equation}
   C(\beta) =  -\sum_i^n \left( y_i \log[p(y_i=1|x_i, \beta)] + (1-y_i)\log [1 - p(y_i = 1|x_i, \beta)]\right)
   \label{eq:ml}
\end{equation}
which can be simplified by noting that 
\begin{equation}
    p(y=1|\boldsymbol{x}, \boldsymbol{\beta}) = \frac{\exp (\boldsymbol{\beta}^T\boldsymbol{x})}{1 + \exp (\boldsymbol{\beta}^T\boldsymbol{x}) }
\end{equation}
Inserting this into equation \ref{eq:ml} and rearranging, we get
\begin{equation}
    C(\beta) = - \sum_i^n y_i (\beta^T x) -\text{log}[1 + \text{exp}(\beta^T x)])
\end{equation}
This is called the cross-entropy and will also be used for the neural network with some slight modifications. The cost function does not have an analytic expression for the minimum. It is therefore necessary to use the numerical optimization methods discussed. However, the cross-entropy is a convex function for the logistic regression problem.\cite{hastie} This assures us that if we find a minima, this is in fact a global minima.\\
\\
The next step is to find the derivatives of the cross entropy. They are simply given by
\begin{equation}
    \frac{\partial C(\beta)}{\partial \beta_i} = - \sum \left( x_i y_i - x_ip_i \right)
\end{equation}
where $p_i = p(y=1|x_i, \beta)$. The expression can be simplified by introducing some new notation. 
We often collect all of the samples in a matrix X, where 
\begin{equation}
    X = \begin{bmatrix}
    1 & x_{11} & x_{12} & ... & x_{1p} \\
    1 & x_{21} & x_{22} & ... & x_{2p} \\
    ... & ... & ... & ... & ... \\
    1 & x_{n1} & x_{n2} & ... & x_{np}
    \end{bmatrix} \in \mathbf{R}^{n \times p+1}.
\end{equation}
n is the number of samples and p is the number of features. 
Here $x_{ij}$ is the j'th feature of the i'th sample. The gradient of the cost function can then be written in matrix notation
\begin{equation}
    \nabla \boldsymbol{C}(\boldsymbol{\beta}) = -X^T(\boldsymbol{y} - \boldsymbol{p})
\end{equation}
where 
\begin{equation}
    p = \frac{1}{1 + \text{exp}(- X\boldsymbol{\beta})}.
\end{equation}
\\
Using gradient descent, the minimums can be found by iteratively updating the approximations of the optimal parameters according to 
\begin{equation}
    \boldsymbol{\beta}^{k+1} = \boldsymbol{\beta}^k +\alpha X^T(\boldsymbol{y}-\boldsymbol{p}).
\end{equation}
for k = 0,1,2,...,K.
% \subsection{ONEHOT ENCODING}
% Onehot encoding is a way to handle categorical variables. Most machine learning algorithms does not have the ability to use categorical variables when training, so we have to code them to useable encodings. 

\subsection{Neural networks}
A neural network is used for both regression problems and classification problems. By varying the arcitechture of the network it can be used to classify images, predict upcoming words in a sentence, or generate samples from a distribution. The network is built up of layers and nodes. Each layer consists of nodes, and nodes are connected to nodes in the neighbouring layers. This forms a network of nodes. Each connection between each layer is associated with a weight. This lets information flow through the network. When information hits a node, we say that the node is activated. We will study a dense feed forward neural network. In a feed forward neural network, data can only flow in one direction. In a dense neural network, every node in one layer is connected to every node in the next layer. A neural network with 3 input nodes, one hidden layer with 6 nodes and one output layer with 1 node is illustrated in figure \ref{fig:nn_illustration}.
\\
\begin{figure}[h!]
    \centering
    \includegraphics[height = 5cm]{nn_ilustration.png}
    \caption{Illustration of a neural network with one hidden layer. This is typically used for regression or binary classification since we only have one output node.Weighted input z comes in to each layer, and then an activation function is applied and gives a.}
    \label{fig:nn_illustration}
\end{figure}
\\
To derive mathematical expression for the neural network, we will need to introduce some notation. The input will be on the form $X = (n_\text{inputs}, n_\text{features})$. X is an matrix that contains the features of each sample. $Y = (n_\text{inputs})$, is an vector with the target values for each sample. The activated values of a layer is described by the matrix $a^l$, where l = 1,2,..,L. When input is on the form as described above, $a^l$ is a matrix with dimension $(n_\text{nodes in layer l}, n_\text{nodes in layer l-1})$. The weight between node j in layer (l-1) and node i in layer l, is denoted by $w_{ij}$. The weights values going into layer l is collected in the matrix 
\begin{equation}
    \boldsymbol{w}^l = 
    \begin{pmatrix}
    w_{11} & w_{12} & ... & w_{1k} \\
    ... & ... & ... & ... \\
    w_{l1} & ... & ... & w_{lk}
    \end{pmatrix}
\end{equation}
Each layer, except the input layer, can have a bias vector $\boldsymbol{b}^l = [b_1^l, .., b_m^l]$.
\\
\\
Flow of information through the network follows a feed forward structure. The input layer consists of the features of a sample. By using the weight matrices, we can efficiently compute the activated values for each layer simultaneously for each sample. The activated values are given by applying the activation function to the weighted sum of the activated values in the previous layer. By using the values in the previous layer and the formula 
\begin{equation}
    \boldsymbol{a}^l =  f(\boldsymbol{a}^{l-1}\boldsymbol{W}^l + \boldsymbol{b}^l)
    \label{eq:feed_forward}
\end{equation}
we can calculate the activated values of the current layer. We often denote 
\begin{equation}
    \boldsymbol{z}^l =  \boldsymbol{a}^{l-1}\boldsymbol{W}^l + \boldsymbol{b}^l
\end{equation}
and it is called the weighted input to layer l. \\
\\
For the first hidden layer we have that $\boldsymbol{a}^{l-1} = X$. By applying equation \ref{eq:feed_forward} recursively from the first hidden layer to the output layer, we can calculate the output by $\boldsymbol{a}^L = f(\boldsymbol{a}^{L-1}\boldsymbol{W}^L + \beta^L)$.
\\
\\
The goal is to adjust the weights and biases so that our model predicts the same values as the target values. This is done via an efficient method called backpropagation. This will be discussed in detail later. 

\subsubsection{Layers and activation functions}
When setting up a neural network, we are left with numerous choices on the architecture of the network. There are endless combinations of number of neurons, number of layers and activation functions.  The optimal combination varies from problem to problem and often it comes down to trial and error. 
\\
\\
When choosing activation function for the layers, it is important to use at least one non-linear activation functions for one of the layers. This way we can model non-linear data. In general, there is no specific rule for choosing the right one. However, when switching between regression and classification problems, we have to do certain changes in the output layer. For regression type problems we typically dont need any activation function in the last layer. For classification, we often use an activation function in the last layer that maps the input values into the interval [0,1]. 
\\
\\
There are some common examples of activation functions that are widely used. One of them is the the sigmoid function 
\begin{equation}
    \sigma(z) = \frac{1}{1 + \text{exp}(-z)}.
\end{equation}
The sigmoid is typically used for the output layer in binary classification problems. This is because the function maps values into the interval [0,1]. It can also be used in the intermediate layers. The derivatives of the Sigmoid function is defined everywhere and are well behaved. However, the vanishing of the gradients for large positive or negative values can be a problem. \cite{Hands-On} This can become a problem when training the model. The derivatives are given by
\begin{equation}
    \sigma'(z) = \sigma(z)\left(1 - \sigma(z)\right)
\end{equation}
Another example of activation functions is the ReLu family. ReLu stand for Rectified learning unit, and the standard ReLu function is given by 
\begin{equation}
    f(z) =\max(z, 0).
\end{equation}
It works well in most cases and are fast to compute. An variation of the ReLu function is leaky ReLu function 
\begin{equation}
    f(z)\begin{cases}
    z & z \geq 0 \\
    0.01z & z < 0
    \end{cases}
\end{equation}
\\
When choosing the number of nodes in each layer there are some restrictions on the number of nodes in the input and output layer. The input layer have to have the same amount of nodes as features in the input. The output have to have the same number of nodes as targets we want to predict. For the hidden layers, we can choose the number of neurons as we want. Keep in mind that by choosing the number to high might lead to overfitting and very high computational time. One way to avoid overfitting is by using various regularization techniques like early stopping or adding a regularization parameter to the cost function. We will discuss the latter. It is often advised that a layer does not have more nodes than the previous layer \cite{Hands-On}, but this is not a rule that necessarily have to be followed. 
\\
\\
The choice of number of hidden layers also comes down to trial and error. One used method is to start with one layer and then gradually add layers until the networks start to overfit. In this project we will use network with 1 to 6 hidden layers. The universal approximation theorem states that by using a neural network with one hidden layer with a finite number of nodes we can approximate any function we want \cite{Hands-On}. 
\subsubsection{Cost function}
When training the model, we need to have control over how good our model fit the data. This is done with the cost function. The idea is to measure the deviations between the predicted values and the target values. 
The cost function is chosen according to the problem one want to solve.
We will study a regression problem and a binary classification problem. For the regression problem, we will use the mean squared error. The MSE is given by 
\begin{equation}
    MSE = \frac{1}{n}\sum_i^n (a^L_i - t_i)^2
\end{equation}
where $t_i$ is the target value of the i'th sample. For binary classification problems we typically use the cross entropy introduced for logistic regression. The cross entropy is given by 
\begin{equation}
    -\sum_i^n[ t_i\log(a^L_i) + (1-t_i)\log(1-a^L_i)]
\end{equation}
where $a_i^L$ is the output from the network and $t_i$ is the actual repsonse of the samples. 
\subsubsection{Backpropagation}
Backpropagation is a efficient method for computing the gradient of the cost function with respect to weights and biases. By adjusting the weights and biases appropriately, the cost function can be minimized. We initialize the weights and biases with small random numbers. It is important that they are not equal to zero, because the gradient would then be zero, and the algorithm would stop. By finding the gradient of the cost-function, we can use stochastic gradient descent for finding the minimum. The derivation is based on the derivation in Morten Hjort-Jensen's lecture notes linked in the start of this section. 
\\
\\
Let's say we have a cost-function given by 
\begin{equation}
    C = \frac{1}{2}\sum_i^n [t_i - a^L(x_i)]^2
\end{equation}
where n is the total number of training samples, and $a^L(x_i)$ is the output vector for sample i and $t_i$ is the target value for sample that we want our model to produce. 
The first step is then to compute the output of the network. We need to know how close we are to the actual values. We start with the first hidden layer and recursively calculate the next layers according to
\begin{equation}
    z_j^l = \sum_{i=1}^{M_{l-1}}w_{ij}^l a_i^{l-1} + b_j^l
\end{equation}
\\
This can be rewritten as 
\begin{equation}
    \boldsymbol{z}^l = (W^l)^T \boldsymbol{a}^{l-1} + \boldsymbol{\beta}^l.
\end{equation}
in vector notation.
\\
For each layer, the activated values are calculated using the activation function of layer l,
\begin{equation}
    \boldsymbol{a}^l = f^l(z^l). 
\end{equation}
For the first hidden layer $a^{l-1}$ is simply the input $\boldsymbol{x}$,
\begin{equation}
    \boldsymbol{z}^1 = (W^1)^T\boldsymbol{x} + \boldsymbol{\beta}^1.
\end{equation}
The next step is to calculate the derivatives of the cost function with respect to the weights and biases of the network. Those are the parameters that we want to change to minimize the cost function. We first note that  
\begin{equation}
    \frac{d z_j^l}{d w_{ji}^l} = a_i^{l-1}
\end{equation}
and  
\begin{equation}
    \frac{\partial z_j^l}{\partial a_i^{l-1}} = w_{ji}^l.
\end{equation}
 Finding the expression for the derivative of the cost function gives that
\begin{equation}
    \frac{\partial C(W^L)}{\partial w_{jk}^L} = \frac{\partial C(W^L)}{\partial a_j^L}\frac{\partial a_j^L}{\partial w_{jk}^L} = (a_j^L - t_j)\frac{\partial a_j^L}{\partial w_{jk}^L} = (a_j^L - t_j)\frac{\partial a_j^L}{\partial z_j^L}\frac{\partial z_j^L}{\partial w_{jk}^L}.
\end{equation}
where the two last steps comes from the chain rule and that 
\begin{equation}
    \frac{\partial C(W^L)}{\partial a_j^L} = (a_j^L - t_j).
\end{equation} 
The expression for $\partial a_j^L/\partial z_j^L$ depends on the activation function, and can be written as 
\begin{equation}
    \frac{\partial a_j^l}{\partial z_j^l} = f'(z_j^l) 
\end{equation}
for an arbitrary layer. By combining this, we get that 
\begin{equation}
     \frac{\partial C(W^L)}{\partial w_{jk}^L} = (a_j^L - t_j) f'(z_j^L) a_k^{L-1}
\end{equation}
where $(a_j^L - t_j)$ is the derivative of the cost function with respect to the activated values in the output layer. This will actually be the same when using the cross entropy as cost function instead of the squared error. 
\\
\\
Next, we introduce the error of the output layer as 
\begin{equation}
    \delta_j^L = f'(z_j^L)\frac{\partial C}{\partial a_j^L}
\end{equation}
and note that
\begin{equation}
    \delta_j^L = \frac{\partial C}{db_j^L}
\end{equation}
Define
\begin{equation}
    \delta_j^l = \frac{\partial C}{\partial z_j^l}
\end{equation}
\\
By using the previous results, the error in a layer can be found by using the error in the next layer (We work our way from the end to the start of the network). 
We can use the chainrule again, to get the expression
\begin{equation}
    \delta_j^l = \sum_k \frac{\partial C}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{\partial z_j^l}.
\end{equation}
and this can be rewritten as
\begin{equation}
    \delta_j^l = \sum \delta_k^{l+1}w_{kj}{l+1}f'(z_j^l)
\end{equation}
by using the previous results.
\\
\\
Calculating all of the derived expression is quite time consuming. Especially in python where the for-loops needed to calculate all the sums and recursive steps would be time consuming. By instead introducing matrix and vector notation, we are able to speed up computations and make the algorithm more efficient. 
The backpropagation algorithm can be summarized as follows using matrix and vector notation. 
\begin{itemize}
    \item Initialize all the weights for each layers randomly. They should be chosen from a normal distribution with zero mean. 
    \item Feed the input through the network and compute the activated values for each layer, including the output layer. 
    \item Compute the error in the output layer,  $\boldsymbol{\delta}^L = (\boldsymbol{t - y})\circ f'(\boldsymbol{z}^L)$. Find the gradient of the cost function with respect to $W^L$ and $b^L$. $$\frac{\partial C}{\partial w^L} = (\boldsymbol{a}^{L-1})^T\boldsymbol{\delta}^L $$ and $$\frac{\partial C}{db^L} = \sum_i^{n^L} \boldsymbol{\delta}^L$$ where the sum is over the samples. 
    \item Propagate the error backward through the network with
    \begin{equation}
        \boldsymbol{\delta^l} = \boldsymbol{\delta}^{l+1}(W^{l+1})^T\circ f'(\boldsymbol{z}^l)
    \end{equation}
     for l = l-1, l-2, ..., 2.
     Find the gradient of the cost function with respect to $W^{l}$ and $b^{l}$. $$\frac{\partial C}{\partial w^l} = (\boldsymbol{a}^{l-1})^T\boldsymbol{\delta}^L $$ and $$\frac{\partial C}{db^l} = \sum_i^{n^l} \boldsymbol{\delta}^l$$ where the sum is over the samples. 
\end{itemize}
The weights and biases are updated with respect to the gradients found in the backpropagation algorithm. Using the stochastic gradient descent method discussed previously, the weights can be updated
\begin{equation}
w^l = w^l - \alpha \frac{\partial C}{\partial w^l}
\end{equation}
and 
\begin{equation}
    b^l = b^l - \alpha \frac{\partial C}{db^l}
\end{equation}
for $l = 2,..., L$
\\
\\
If the error is propagated to a large stack of layers, the backpropagation might encounter vanishing gradients. This makes it impossible or very slow to train the network any further, and the weights will be stuck. 
\\
\\
Details on the implementation of the backpropagation algorithm are found in the class \lstinline{NN} in the python file \lstinline{nn.py}. The file also contains classes for creating each layer and a class for setting up the whole neural network. 
\subsubsection{Regularization}
One of the main problems with neural networks is overfitting.\cite{Hands-On} Especially for networks with a large number of weights and biases. Overfitting the data means to fit the model to noise instead of the actual signals. By doing this, the model fit the traning data very well, but when tested on unseen data it performs poorly. There is different ways to address the problem of overfitting. Regularization is the method used in this project. The idea is that by using regularization, we can maintain the size of the network without overfitting the data.  This is done by restricting the size of the weights by adding an extra penalty term to the cost function. \cite{Hands-On} This way, the network can not choose the parameters as large as they would like. By changing the cost function to 
\begin{equation}
 C(\theta) = \sum_i c_i(x_i, \theta_i) \to \sum_i c_i(x_i, \theta_i) + \lambda \sum_{ij} w_{ij}^2   
\end{equation}
we can obtain the desired results. $\lambda$ is the penalty parameter. The neural network class in \lstinline{nn.py} contains an option to train the network with or without the regularization parameter. Details are found in the docstring of the method. 

\subsection{Ridge regression}
Ridge regression is an extension of the simple OLS-model. It adresses one of the main problems with OLS, which is the occurence of non-invertible $(X^TX)$. We will also look at Lasso regression which is another example of methods that builds on the OLS-model. These are regularization techniques, and are also used to reduce the variance of the model. By adding a restriction to the coefficients, we are able to reduce the degrees of freedom, and thereby the variance. By reducing the coefficients in the regression, the goal is to reduce the variance, while still keeping the bias low.\cite{hastie} This is done by adding a penalty parameter to the minimisation problem. 
The minimization problem now becomes
\begin{equation}
    \hat{\beta}_{\text{ridge}} = \text{argmin}_\beta\left(\sum_{i=1}^N(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda\sum_{j=1}^p\beta_j^2 \right)
    \label{eq:ridge_min}
\end{equation}\cite{hastie}
One important part of this equation is the exclusion of the intercept $\beta_0$ in the last term. This problem can be rewritten to matrix notation, so we see that \ref{eq:ridge_min} is the same as minimizing
\begin{equation}
(\boldsymbol{y - X\beta})^T(\boldsymbol{y - X\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}.
\label{eq:ridge_matrix_eq}
\end{equation}
with respect to $\boldsymbol{\beta}$,
where $\lambda$ is a regularization parameter. By setting $\lambda$ to zero, we get the normal least squares estimator. By increasing $\lambda$ we can shrink the coefficients. $\lambda$ can therefore be used to control the size of the coefficients. 
To find the optimal $\beta$, we take the derivative of equation \ref{eq:ridge_matrix_eq}, and find the value of $\boldsymbol{\beta}$ that gives the derivative equal to zero. 
\begin{equation}
    -2\boldsymbol{X}^T(\boldsymbol{y - X\beta}) + 2\lambda \boldsymbol{\beta} = 0
\end{equation}
By rearranging the equation, the least-squares estimator becomes 
\begin{equation}
    \boldsymbol{\hat{\beta}}_{\text{ridge}} = (\boldsymbol{X}^T\boldsymbol{X} + \lambda \textbf{I})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation}
where $\boldsymbol{\textbf{I}}$ is the identity matrix. This can be implemented in python with the following code
\begin{lstlisting}
beta_ridge = np.linalg.inv(design_matrix.T.dot(design_matrix) + \
    lam*np.eye(len(design_matrix[1,:]))).dot(design_matrix.T).dot(target_values)
\end{lstlisting}
We can find the variance of the coefficients by using a similar technique as for OLS. 
\begin{equation}
\begin{split}
    \text{Var}(\hat{\boldsymbol{\beta}}_\text{ridge}) = & \text{Var}((\boldsymbol{X}^T\boldsymbol{X} + \lambda \textbf{I})^{-1}\boldsymbol{X}^T\boldsymbol{y}) \\
    = & (\boldsymbol{X}^T\boldsymbol{X} + \lambda \textbf{I})^{-1}\boldsymbol{X}^T\text{Var}(\boldsymbol{y})((\boldsymbol{X}^T\boldsymbol{X} + \lambda \textbf{I})^{-1}\boldsymbol{X}^T)^T \\
    = & \sigma^2 (\boldsymbol{X}^T\boldsymbol{X} + \lambda \textbf{I})^{-1}\boldsymbol{X}^T ((\boldsymbol{X}^T\boldsymbol{X} + \lambda \textbf{I})^{-1}\boldsymbol{X}^T)^T \\
    = & \sigma^2 (\boldsymbol{X}^T\boldsymbol{X} + \lambda \textbf{I})^{-1}\boldsymbol{X}^T \boldsymbol{X} ((\boldsymbol{X}^T\boldsymbol{X} + \lambda \textbf{I})^{-1})^T
\end{split}
\end{equation}
This can be implemented in python with the following code
\begin{lstlisting}
beta_var = np.diag(sigma_2*(np.linalg.inv(X.T@X + self.lam*np.eye(len(X[1,:])))@X.T@X@(np.linalg.inv(X.T@X+self.lam*np.eye(len(X[1,:])))).T))
\end{lstlisting}
We see from the equation that the variance of the coefficients decrease when we increase lambda. 
Similar to what was done for OLS, we can find confidence intervals for each coefficient. The 95\% confidence interval are given by 
\begin{equation}
    \left[\beta_i - 1.96\sqrt{\text{Var}(\beta_i)}, \beta_i + 1.96\sqrt{\text{Var}(\beta_i)}\right]
\end{equation}
Now that we have expressions for the variance of the coefficients for both OLS and ridge, we can look at the difference between the two of them. 
\begin{equation}
    \text{Var}(\beta_{\text{OLS}}) - \text{Var}(\beta_{\text{ridge}}) = \sigma^2[X^TX + \lambda I]^{-1}[2\lambda I + \lambda^2(X^TX)^{-1}][X^TX + \lambda I]^T
\end{equation}
This expression is always positive, which means that as long as $\lambda > 0$, the variance of the coefficients for OLS are larger than for Ridge.\cite{hastie} 
\\
\\
When finding the optimal complexity for ridge regression, there are now to parameters that have to be tuned. We have to look at both the value of lambda and the polynomial degree.  
\subsection{Lasso regression}
Lasso regression is another regularization method. It is quite similar to the ridge regression, but uses the $\ell_1$-norm in the penalty term. The minimisation problem is now given by 
\begin{equation}
    \hat{\boldsymbol{\beta}}_{\text{lasso}} = \text{argmin}_\beta\left( \frac{1}{2} \sum_{i=1}^N( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j|\right).
\end{equation}\cite{hastie}
\\
This gives us an challenge when we try to find the optimal $\boldsymbol{\beta}$. We are no longer able to find an analytical solution, so we have to use numerical optimization techniques.\cite{Hands-On} One of the easiest methods is the gradient descent method. The gradient descent method minimizes the MSE by repeatedly finding the gradient of the cost function with respect to $\boldsymbol{\beta}$, and then move in the opposite direction of the gradient for some distance. This distance is called the learning rate. This have to be chosen carefully for the algorithm to converge in reasonable time. One way to do this is to test for different fixed values and find out which one is best. Line-search methods are an alternative, where the optimal learning rate are found for each gradient.\cite{NumOpt} Scikit-learn have this functionality built in. Since lasso dont have a closed form solution to the beta problem, it is a bit more tricky to implement. For this report we have therefore used Scikit learns functionality to fit the Lasso regression. Predictions for the Lasso regression in python can be implemented with the following code
\begin{lstlisting}
from sklearn.linear_model import Lasso
Lasso_reg = Lasso(alpha = lambda_value)
Lasso_reg.fit(design_matrix, targets)
predictions = Lasso_reg.predict(desing_matrix_2)
\end{lstlisting}
The main difference when it comes to the regression coefficients we get, are that lasso are able to make some of the coefficients equal to zero. This means that Lasso can actually be used for feature selection, and are able to remove features that are not relevant to the regression.\cite{hastie} 

\subsection{Comparing the models}
To compare the different models, we have to use an error-estimate. For regression type problems, we typically use the mean squared error, denoted by MSE.\cite{hastie} The mean squared error measures the mean of the squared errors, where the errors is the difference between the predicted value and the actual value. The MSE says something about how good we are able to model the data. The lower the score, the better the model. The MSE can be written as 
\begin{equation}
    MSE = \text{E}[(y - \hat{y})^2] = \frac{1}{n}\sum_{i=0}^n(y_i - \hat{y}_i)^2
\end{equation}
where y is a vector of the actual target values, and $\hat{y}$ is a vector of the models predicted values. The MSE-error can be implemented in python with the following code: 
\begin{lstlisting}
MSE = sum((y_pred - y)**2)*(1./n)
\end{lstlisting}
If the model interpolates all the data points, the MSE is equal to zero. This means that zero MSE is equal to perfect fit. 
\\
\\
Another measure that can be used to compare the models are the $R^2$-value, also called the coefficient of determination. The $R^2$-value is a measure of how much of the variance in the data we are able to explain with our regression.\cite{hastie} The $R^2$-score goes from zero to one, and we want the score to be as close to one as possible. If we get an R2-value that is close to one, it is able to explain a lot about the variability in the data. This probably means that we have chosen the right complexity. If its close to zero, it might be evidence that we use the wrong complexity. For example that we are trying to fit  highly polynomial data with a linear model. There might also be that there is a lot of noise in the data, so that the so called irreducible error is high. The function for calculating the $R^2$-value are given by
\begin{equation}
    R^2 = 1 - \frac{\sum_i(y_i - \hat{y}_i)}{\sum_i(y_i - \bar{y})}
\end{equation}
\\
and can be implemented in python with the following code:
\begin{lstlisting}
R2 = 1 - (sum((y - y_pred)**2)/sum((y - y_bar)**2))
\end{lstlisting}
There are different ways to measure those values. We can for example do it on the training data, and find the training MSE and the training R2. This is however not the usual way to do it. We are more interested in the generalization error or the test error, instead of the training error. The test error is measured on a new and independent data set, which the model has not been trained on. We will very often overestimate the models capabilities for prediction if we use the training error as the measurement of goodness of fit. This is because that by increasing the complexity of the model, we can almost get the line to interpolate our data. But this is not necessarily a good thing, if we try to do predictions on a new data set. Therefore, we use the test error to choose our model. One way to measure the test error is to split the data into three parts; a training part, a test part and a validation part. However, we often do not have access to large amounts of data. By splitting it up in many parts, we get less data to train our model on, and the fit is typically worse than if used more data. Instead we can use resampling techniques. These techniques are able to estimate the test error for smaller sets of data. 
\subsection{Resampling methods}
Resampling methods is a way for estimating the test-error of a model. One way to do this is by a  cross-valdiation. The idea is to randomly split our data into folds, leave one of the folds as a unseen test-set, and then train our model on the remaining folds of data.\cite{hastie} We then calculate the MSE for the test data, and repeat the procedure, but now choosing another fold as test data. Taking the mean of the k MSE's gives the CV-error. This can be summarized in the following list:

\begin{enumerate}
    \item Split the data into k-folds. It is important that this split is random. 
    \item Leave one of the folds out and train the model on the remaining folds.
    \item Estimate the test-error on the left-out fold.
    \item We repeat step 2 and 3 k times for different fold, and calculate the prediction error for each.
    \item Take the mean of the previously measured prediction errors. This is the CV-error. 
\end{enumerate}
The cross-validation error can be seen as an estimate of how well we can expect our model to perform on unseen data. 
\\
The k-fold cross validation can be implemented in python with the following code
\begin{lstlisting}
k_folds = KFold(n_splits = num_folds,shuffle=True)
fold_score = np.zeros(num_folds)
i = 0
for train_index, test_index in k_folds.split(data):
    x_train = data[train_index]
    x_test = data[test_index]
    y_train = response[train_index]
    y_test = response[test_index]
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    fold_score[i] = MSE(y_pred, y_test)
    i+=1
return np.mean(fold_score)
\end{lstlisting}
Typical values of k are 5 and 10. In this report we will use k=5. We have to keep in mind that the choice of k leads to different bias and variance of the estimator. By choosing 5 we decrease the variance compared to 10, but we might get some more bias.\cite{hastie}

\subsection{Bias-Variance trade-off}
One of the problems with fitting machine learning models, is the choice of complexity. The complexity of a model can for example be the degree of polynomial we fit the data with in a polynomial regression. This problem can be summarized in the bias-variance trade-off. 
Training error will often underestimate the true error, because it uses the same data both for fitting and for validation.\cite{hastie} 
\\
\\
We can use the mean-squared error, MSE, to get an idea about how good our model is. The MSE is given by 
\begin{equation}
MSE = \boldsymbol{E}[(\boldsymbol{y} - \boldsymbol{\hat{y}})^2].
\end{equation}
\\
This can be used to study the bias-variance trade-off. This is an important effect we have to be aware off when fitting a model. I will start with a rewriting off the MSE, and then explain the effects of the bias-variance trade-off. 
\\
We see that we can write 
\begin{equation}
MSE = \boldsymbol{E}[(\boldsymbol{y} - \boldsymbol{\hat{y}})^2] = \frac{1}{n}\sum_i(f_i - E[\boldsymbol{\hat{y}}])^2 + \frac{1}{n}\sum_i(\hat{y}_i - E[\boldsymbol{\hat{y}}])^2 + \sigma^2.
\label{eq:bias_variance}
\end{equation}
The expression can be derived by using that 
\begin{equation}
    \boldsymbol{y} = \boldsymbol{f + \epsilon}
\end{equation}
By adding and subtracting $\text{E}(\hat{y})$ we get that
\begin{equation}
\begin{split}
\text{E}[(\boldsymbol{y - \hat{y}})^2] = & \text{E}[((\boldsymbol{y - \text{E}(\hat{y}})) - (\hat{\boldsymbol{y}} - \text{E}(\boldsymbol{\hat{y}})))^2]\\
= & \text{E}[(y-\text{E}(\hat{y}))^2] - 2\text{E}[(y-\text{E}(\hat{y}))(\hat{y}-\text{E}(\hat{y}))] + \text{E}(\hat{y} - \text{E}(\hat{y})^2) \\
= & \text{E}[(y-\text{E}(\hat{y}))^2] + \text{E}[(\hat{y} - \text{E}(\hat{y}))^2]
\end{split}
\end{equation}
The last step is because
\begin{equation}
2\text{E}[(y-\text{E}(\hat{y}))(\hat{y}-\text{E}(\hat{y}))] = 0
\end{equation}
since the two factors of the equation are independent, and 
\begin{equation}
\text{E}(\hat{y}-\text{E}(\hat{y})) = 0.
\end{equation}
Further, we can show that 
\begin{equation}
\text{E}[(\hat{y} - \text{E}(\hat{y}))^2] = \text{Var}(\hat{y}) = \frac{1}{n}\sum_i(\hat{y}_i - \text{E}(\hat{y}))^2.
\end{equation}
By showing that
\begin{equation}
\begin{split}
\text{E}[(y-\text{E}(\hat{y}))^2] = & \text{E}[ (f+\epsilon)^2 - 2(f+\epsilon)\text{E}(\hat{y}) + \text{E}(\hat{y})^2  ] \\
= & \text{E}[(f^2 - 2f\text{E}(\hat{y}) + \text{E}(\hat{y})^2) + 2f\epsilon + \epsilon^2 - 2\epsilon\text{E}(\hat{y}) ]
\end{split}
\end{equation}
Since 
\begin{equation}
\text{E}(f^2 - 2f\text{E}(\hat{y}) + \text{E}(\hat{y})^2) = \text{E}((f-\text{E}(\hat{y}))^2) = \frac{1}{n}\sum_i(f_i - \text{E}(\hat{y}))^2
\end{equation}
we can use that
\begin{equation}
    \text{E}(2f\epsilon) = \text{E}(\epsilon\text{E}(\hat{y})) = 0
\end{equation}
and
\begin{equation}
    \text{E}(\epsilon^2) = \text{Var}(\epsilon) + \text{E}(\epsilon)^2 = \text{Var}(\epsilon) = \sigma^2
\end{equation}
to get 
\begin{equation}
\text{E}[(y - \hat{y})^2] = \sigma^2 + \frac{1}{n}\sum_i(f_i - \text{E}(\hat{y}))^2 + \frac{1}{n}\sum_i(\hat{y}_i - \text{E}(\hat{y}))^2
\label{bias_variance_eq}
\end{equation}
The equation is a sum of an irreducible part and a reducible part. The irreducible part is the part of the error that we can not remove.\cite{hastie} This part is "built" in to the data, and can for example be a result of failing equipment for measuring. The irreducible error is represented by $\sigma^2$.  The reducible part is a result of model choice, and we can reduce it by tweaking the model.\cite{hastie} The reducible part consist of bias and variance. The bias is the second term in equation \ref{bias_variance_eq} and the variance are the third term. The bias comes from wrong assumptions about the model, while variance comes from how sensitive the model is to changes in the data. This is quantities that can be tweaked by choosing the correct model complexity. 
When fitting a model, we have do a trade-off between bias and variance. When we increase the complexity, the model we get are very data-dependent. By changing the training data-set, our model can change drastically. The model will vary a lot if we change the training data. This means that when we test our model on the test set, it might be to specialized for the training data, and will not give a good prediction. However, when the complexity is this high, the model is free to fit to the shape of the training data, so the bias is low. On the other hand, when we have a low model complexity, we get high bias and low variance. The high bias comes from the fact that we might try to model complex data with for example a linear model. 
\\
\\
We can avoid overfitting by using more data when fitting the model. We will see that the test error improves when we add more and more data to the model. Overfitting occurs when we tune our model to well to the training data, so that it does not work well on unseen data.  \\
\subsection{Preprocessing}
There is often useful to scale the data before we do a regression. Especially for the lasso regression, which uses a gradient descent, we can often improve convergence by scaling \cite{NumOpt}. Since we want to compare the methods, it will be useful to scale the data for all types of regression, to get a meaningful relationship between the different methods. We could also have rescaled the data back by saving the scaling factors. However, in this report we are not that interested in the exact scale of the target values. Instead we focus on how got our model fit the data. Scaling is also useful to avoid outliers.
Scaling can be done by subtracting the means and dividing by the standard deviation of the data.\cite{Hands-On} Implementation of scaling in python has been done using the built in Sklearn function scale. It was done before fitting and resampling. 
\begin{lstlisting}
from sklearn.preprocessing import scale
X = scale(X)
z = scale(z)
\end{lstlisting}
However, another, and probably better idea,is to scale both the training part and the test part after the splitting of the data, with the mean and std. of the training part. This way we will get the correctly scaled data fitted. If we scale all the data first and then pick out a training part, this is no longer necessarily scaled. This might be a good idea to try to implement at a later stage. The results seemed fine for the way I did it, but one might think that the MSE's could be improved further by doing the scaling the correct way. 



\subsection{Measures of performance}
Choosing the right model is an important problem when applying machine learning to classification and regression problems. No algorithm is the best for every problem, and there is in general no specific rule for choosing the right one. For each model we can typically also fine-tune parameters, so the space to choose from becomes very large. It is therefore important to have ways of comparing the different models. Finding the best model comes down to testing different models on the data, and comparing their performance to each other. However, one have to restrict the search space to a manageable size.  We then have to introduce metrics to determine which one is the best for the specific case.  
\subsubsection{Measures of performance for classification}
Classification often deals with labeling samples into a set of different categories. The goal is to classify as many samples as possible to the actual category of the sample. There are different methods for measuring the classifiers ability to classify points. Some of them are presented in this section. All of the methods assumes that we have labeled test data. 
\\
\\
An often used performance measure for classification problems is the accuracy score. The accuracy score is the proportion of correctly classified data points. It is given by
\begin{equation}
    \text{Accuracy} = \frac{1}{n}\sum_i^n I(y_i = t_i). 
\end{equation}
where $y_i$ is the predicted value of sample $i$, $t_i$ is the target value of sample $i$ and $n$ is the number of samples. $I(y_i = t_i)$ is the indicator function,
\begin{equation}
    I(y_i = t_i) = \begin{cases}
    1 & ,\text{ } y_i = t_i \\
    0 & ,\textit{ } y_i \neq t_i
    \end{cases}
\end{equation}
The accuracy is easy to interpret and implement, and works fine if the data is unbiased. However, if the data is biased, there is a problem with using the accuracy. Let's say that the targets consist of 90$\%$ one's. A model that only predicts ones will then get an accuracy of 90$\%$. Choosing a model based on the accuracy can therefore lead to choosing a model without the ability to classify data. 
\\
\\
An better alternative is to check how many points the model correctly classifies and how many it misclassifies. For a binary classification problem, we can start by introducing some notation \cite{Hands-On}.  
\begin{itemize}
    \item TP: True positive. The number of 1's classified as 1's. 
    \item FP: False positive. The number of 0's classified as 1's. 
    \item TN: True negative. The number of 0's classified as 0's. 
    \item FN: False negative. The number of 1's classified as 0's.
    \item TPR: True positive rate. $TPR = TP/P$. Also called sensitivity. 
    \item FPR: False postive rate. $FPR = 1 - TN/N$. $TN/N$ is called the specificity. 
\end{itemize}
A confusion matrix is a way to summarize the measures in a compact way. It gives a quick overview of how well the model performs.  In addition, other measures can be calculated by using the values in the matrix. Each column represents the true label of the data, and each row represents the predicted label. A general set-up for a confusion matrix is illustrated in table \ref{tab:confusion_matrix}. The ideal would be a case where FP and FN are equal to zero. We want those as small as possible. 
\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
         & 1 & 0 \\ \hline
        1 & TP & FP \\
        0 & FN & TN 
    \end{tabular}
    \caption{Illustration of a confusion matrix for binary classification. Columns are the true values, while rows are the predicted values.}
    \label{tab:confusion_matrix}
\end{table}
\\
Using the previously calculated values, we can also find the roc-curve. It plots the true positive rate (TPR) against the false positive rate (FPR) for different threshold values \cite{Hands-On}. An optimal classifier would have the curve hugging the upper left corner. The models can be compared by calculating the area under the roc-curve, often called the AUC-score. The higher the AUC-score, the better the model. 
\\
\\
Another measure used is the area ratio. To calculate the area ratio we need to find the cumulative gains curve. The cumulative gains curve plots the percentage of classifications of a given class as a function of number of total samples. An optimal model would have a linear curve from (0,0) to (1, percentage of class in total data). The baseline curve is a straight line from (0,0) to (1,1). When the three curves are calculated, the area ratio is given by 
\begin{equation}
    \text{Area ratio} = \frac{\text{area between baseline curve and the models curve}}{\text{area between the baseline curve and the optimal curve}}
\end{equation}
% The optimal area ratio is one. In that case the cumulative gains curve of the model is the same as the one for the optimal classifier. For more info about the cumulative gains curve see.\footnote{https://www.ibm.com/support/knowledgecenter/de/SSLVMB_24.0.0/spss/tutorials/mlp_bankloan_outputtype_02.html}
\subsubsection{Measures of performance for regression}
For regression problems we will use the mean squared error (MSE) for comparison. The MSE is given by
\begin{equation}
    MSE = \frac{1}{n}\sum_i^{n}(y_i - t_i)^2. 
\end{equation}
where $y_i$ is the predicted value of sample i, $t_i$ is the target value of sample i, and $n$ is the number of samples. 
The MSE measures the average of the squared deviations. We want the MSE as low as possible. In this case most of our predictions are close to the target values. 

\section{Unsupervised learning}
The next area of machine learning that we will discuss is the unsupervised learning algorithms. The thing that characterzis nsupervised learning algorihtms is that we no longer have to have labeled data to feed into our model. Now, we can instead try to make the models learn from the data by themselves. 

\subsection{Energy based models}
An energy based model have som way of describing the energy of the system. It should not necessarily by confused with the physical energy of the system. However, in this work, we will try to minimize the energy of our model. 

\subsection{Restricted Boltzmann Machines}
A Boltzmann machine is a generative and unsupervised machine learning model. For a general introduction to machine learning, see project 2 from FYS-STK4155 \cite{4155}. The idea behind using a generative model is that it can be used to sample from a probability distribution \cite{mhj_ml}. As for most machine learning models, gradient based optimization methods are used for finding the weights and biases that minimizes a cost function. However, in unsupervised learning methods, there are no labeled data. 
\\
\\
While a standard deep neural network often have multiple layers, the Boltzmann machine only has two. Multiple Boltzmann machines can however be connected \cite{mhj_ml}, called a deep belief network. In this article a single Boltzmann machine with two layers are used. In general, the Boltzmann machine contains a hidden layer and a visible layer. Each node in the layers are connected to every other node. In addition, each node can have its own bias. The main problem with the Boltzmann machine is that they are hard to train \cite{mhj_ml}. As a way to fix this, connections between nodes in the same layer can be removed. The new network is then called a restricted Boltzmann machine \cite{mhj_ml}.
\\
\\
A restricted Boltzmann machines is used to represent the wave function. It is a special case of a Boltzmann machine, where all the nodes are only connected to nodes in the opposite layer \cite{mhj_ml}.  A restricted Boltzmann machine can be illustrated by the diagram in figure \ref{fig:bm}. Note that there are no connections between the nodes in the same layer. The illustration if for a network with 4 visible nodes and 3 hidden nodes. This can easily be extended to more nodes in each layer. In addition, there are biases that are added to each hidden and visible node. 
\begin{figure}[h!]
    \centering
    \includegraphics[width = \textwidth]{Boltzmann machine.pdf}
    \caption{Illustration of a Restricted Boltzmann Machine with 4 visible nodes and 3 hidden nodes.}
    \label{fig:bm}
\end{figure}
The hidden layer is a vector denoted by $\boldsymbol{h}$ of length N. The visible layer $\boldsymbol{x}$ is of length M. In addition, each layer have a bias vector of the same length as length of the layer, as depicted in the figure. The bias vector of the hidden layer is called $\boldsymbol{b}$ and the bias vector for the visible layer is called $\boldsymbol{a}$. The weights between nodes in different layers will be represented by the matrix $\boldsymbol{W}$ of size $M \times N$. Element $w_{ij}$ is the connection between node i in the visible layer to node j in the hidden layer. For this problem, the visible layer will represent the position of the particles in the system. Each component of the vector represents one particles coordinate. The length of the visible layer for a system with 2 particles in two dimensions is $2\times2 = 4$,
\begin{equation}
    \boldsymbol{x} = [x_{11}, x_{12}, x_{21}, x_{22}]
\end{equation}
where $x_{ij}$ is the j'th coordinate of the i'th particle. The hidden layer is sometimes called the feature vector and the length can be chosen freely. 
\\
\\
To be able to sample from a distribution, the network have to be described by some probability function. The joint probability function of the hidden layer and the visible layer is given by the Boltzmann distribution
\begin{equation}
    P_{rbm}(\textbf{x,h}) = \frac{1}{Z}e^{-\frac{1}{T_0}E(\textbf{x,h})},
\end{equation}
where $T_0$ is set to one, and 
\begin{equation}
    Z = \int\int e^{-\frac{1}{T_0}E(\boldsymbol{x,h}).}
\end{equation}
where Z is the partition function \cite{mhj_ml}. E is the energy of the Boltzmann machines configuration of hidden and visible nodes. This is not the same as the energy of the quantum mechanical system. 
\\
\\
The energy function depends on the type of Boltzmann machine being used. For this project a so called Gaussian-Binary RBM is used. In a Gaussian-Binary RBM, the visible units are Gaussian, while the hidden units are binary \cite{mhj_ml}. It is important to use continuous visible nodes, since they are supposed to represent the position of the particles.  
The energy of the Gaussian-Binary RBM is given by
\begin{equation}
E(\textbf{x,h}) = \sum_i^M\frac{(x_i - a_i)^2}{2\sigma_i^2} - \sum_j^N b_jh_j - \sum_{i,j}^{M,N}\frac{x_i w_{ij}h_j}{\sigma_i^2}.
\end{equation}
where $\boldsymbol{x, h, a, b}$ and $\boldsymbol{W}$ are as described above \cite{mhj_ml}. 
\subsection{Neural-Network Quantum State}
The network is used to learn a probability distribution by updating its weights and biases. In contrast to standard neural networks, the restricted Boltzmann machine does not return any output given some input. Instead, it produces a probability distribution from which we can sample from \cite{mhj_ml}. 
This probability distribution is supposed to represent the wave function. The wave function of the system can be derived from the joint distribution of the Boltzmann machine \cite{mhj_ml}. Since the wave function of the system should depend on the position of the particles, the marginal density of $\boldsymbol{x}$ is the distribution of interest. Integrating out the hidden variable from the joint distribution gives
\begin{equation}
    F_\text{rbm}(\boldsymbol{x}) = \sum_{\boldsymbol{h}}F_\text{rbm}(\boldsymbol{x, h}) = \frac{1}{Z}\sum_{\boldsymbol{h}}e^{-E(\boldsymbol{x,h})}.
\end{equation}
This is what will be used to represent the wave function. 
Inserting the energy-function for the Gaussian-Binary RBM and rearranging, gives
\begin{equation}
    \uppsi(\boldsymbol{x}) = F_{\text{rbm}}(\boldsymbol{x}) = \frac{1}{Z}e^{-\sum_i^M\frac{(x_i - a_i)^2}{2\sigma^2}}\prod_j^N\left(1 + e^{b_j + \sum_i^M\frac{x_iw_{ij}}{\sigma^2}}\right)
\end{equation}
This is called the neural quantum state \cite{carleo}.
\subsection{Cost Function}
Most machine learning algorithms use some kind of cost function. For supervised learning, the cost function is often a measure of difference between the output of the network and the desired output. This means that the training data have to consist of labeled data where the networks parameters are fine-tuned to give the lowest possible difference. For unsupervised learning, this is not the case. Different cost function can be defined for unsupervised learning. For this case the energy of the system is used as the cost function \cite{mhj_ml}, and is the quantity we want to minimize. 
\subsubsection{Local Energy}
The local energy of the system is defined as
\begin{equation}
E_L = \frac{1}{\uppsi}\hat{\boldsymbol{H}}\uppsi
\end{equation}
where $\uppsi$ is the NQS given by the Boltzmann machine \cite{mhj_ml}. It can be shown that
\begin{equation}
E_L = \frac{1}{2}\sum_p^P\sum_d^D\left(-\left(\frac{\partial}{\partial x_{pd}}\ln \uppsi \right)^2 - \frac{\partial^2}{\partial x^2_{pd}}\uppsi + \omega^2x_{pd}^2\right) + \sum_{p<q}\frac{1}{r_{pq}}.
\end{equation}
See the appendix for a full derivation. For the non-interacting system, the last term is zero. 
\\
\\
The formulas for derivatives should be known analytically to speed up computations. 
Taking the logarithm of the wave function gives 
\begin{equation}
    \ln(\uppsi(\boldsymbol{X}) = -\ln(Z) - \sum_i^M\frac{(x_i - a_i)^2}{2\sigma^2} + \sum_j^N\ln(1 + e^{b_j + \sum_i^M\frac{X_iw_{ij}}{\sigma^2}}).
\end{equation}
\\
\\
The derivatives with respect to the visible nodes are given by
\begin{equation}
    \frac{\partial}{\partial x_m}\ln \uppsi = -\frac{1}{\sigma^2}(x_m - a_m) + \frac{1}{\sigma^2} \sum_n^N \frac{w_{mn}}{\exp\left[-b_n - \frac{1}{\sigma^2}\sum_i^Mx_iw_{in}\right] + 1}
\end{equation}
and 
\begin{equation}
    \frac{\partial^2}{\partial x_m^2}\ln \uppsi = -\frac{1}{\sigma^2} + \frac{1}{\sigma^4}\sum_n^N w^2_{mn}\frac{\exp\left[b_n + \frac{1}{\sigma^2}\sum_i^Mx_iw_{in}\right]}{\left(\exp\left[b_n + \frac{1}{\sigma^2}\sum_i^Mx_iw_{in}\right]+1\right)^2}
\end{equation}
\cite{mhj_ml} and can be used for calculating the local energy. 
\subsubsection{Minimizing the Cost Function}
As mentioned, the energy of the quantum mechanical energy is used as a cost function. The goal is to tweak the weights and biases of the network to obtain the lowest possible energy. This is done by optimization methods. One method that is often used in machine learning is the gradient descent algorithm. For a thorough description of gradient based optimization methods and updating of weights in a neural network, see \cite{4155}. The main step of the gradient descent method is to move in the opposite direction of the gradient of the function, where the function f is dependent on some parameters $\theta$. The idea is that since the gradient points in the direction where the function increase fastest, moving in the opposite direction will lead to the fastest decrease in function value. The algorithm goes as follows:
\begin{itemize}
    \item Choose an initial set of parameters.
    \item Continue until stopping criteria:
    \begin{itemize}
        \item Update the parameters according to 
        \begin{equation}
            \theta^{(t+1)} = \theta^{(t)} - \lambda \nabla f(\theta^{(t)})
            \label{gd}
        \end{equation}
    \end{itemize}
\end{itemize}
where $\lambda$ is the learning rate. 
To minimize the cost function (local energy), the gradient is needed. 
The gradients with respect to the weights and biases are given by
\begin{equation}
    G_{\alpha} = \frac{\partial \langle E_L \rangle}{\partial \alpha} = 2\left(\langle E_L \frac{1}{\uppsi}\frac{\partial \uppsi}{\partial \alpha}\rangle - \langle E_L \rangle \langle\frac{1}{\uppsi}\frac{\partial \uppsi}{\partial \alpha}\rangle  \right)
    \label{gradient}
\end{equation}
\cite{mhj_ml}
.The expected values are calculated with Monte Carlo methods using samples from the Markov Chain Monte Carlo methods presented later. Note that
\begin{equation}
    \frac{1}{\uppsi}\frac{\partial \uppsi}{\partial \alpha_i} = \frac{\partial \ln \uppsi}{\partial \alpha_i}.
\end{equation}
The derivatives of the wave function with respect to the biases and weights are needed in the gradient descent updates. For a full derivation, see the appendix. Those are given by 
\begin{equation}
    \frac{\partial}{\partial a_m}\ln \uppsi = \frac{1}{\sigma^2}(X_m - a_m)
\end{equation}
\begin{equation}
    \frac{\partial}{\partial b_n}\ln \uppsi = \frac{1}{\left(\exp \left[-b_n - \frac{1}{\sigma^2}\sum_i^MX_iw_{in}\right]+1\right)}
\end{equation}
and 
\begin{equation}
    \frac{\partial }{\partial w_{mn}}\ln \uppsi = \frac{X_m}{\sigma^2\left(  (\exp \left[-b_n - \frac{1}{\sigma^2}\sum_i^MX_iw_{in}\right]+1 \right)}
\end{equation}
\cite{mhj_ml}.
For Gibbs Sampling an alternative wave function is used, $\uppsi(\boldsymbol{X}) = \sqrt{F_{\text{rbm}}(\boldsymbol{X})}$. This gives slightly different derivatives:
\begin{equation}
    \frac{\partial}{\partial a_m}\ln \uppsi = \frac{1}{2\sigma^2}(X_m - a_m)
\end{equation}
\begin{equation}
    \frac{\partial}{\partial b_n}\ln \uppsi = \frac{1}{2\left(\exp \left[-b_n - \frac{1}{\sigma^2}\sum_i^MX_iw_{in}\right]+1\right)}
\end{equation}
and 
\begin{equation}
    \frac{\partial }{\partial w_{mn}}\ln \uppsi = \frac{X_m}{2\sigma^2\left(  (\exp \left[-b_n - \frac{1}{\sigma^2}\sum_i^MX_iw_{in}\right]+1 \right)}
\end{equation}
\cite{mhj_ml}.
\\
\\
The ground state energy is found when the quantum mechanical energy is at its lowest \cite{mhj_vmc}. The weights are then tweaked according to the gradient descent algorithm to find this.
% \begin{itemize}
%     \item Initialize $x^0$ with some value
%     \item Until convergence:
%     \begin{itemize}
%         \item $x^{t+1} = x^t - \gamma \Delta f(x)$
%     \end{itemize}
% \end{itemize}

\newpage